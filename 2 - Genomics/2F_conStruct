# conStruct (continuous Structure)

# https://github.com/gbradburd/conStruct
# Using Dataset 1
# As input, using the populations.structure output by STACKS populations and formatting it with the script below
# All code (with small modifications, not to the functions) included in the conStruct package 

library(Rcpp)
library(conStruct)
library(geosphere)

### FORMATTING DATA ###
# From STRUCTURE format (Stacks: populations.structure) to conStruct format
conStruct.allele.freq <- structure2conStruct(infile = "populations.structure",
                                             onerowperind = FALSE,
                                             start.loci = 3,
                                             start.samples = 3,
                                             missing.datum = 0,
                                             outfile = "ConStructData")

# Making coordinate matrix of the samples
sample.coords <- read.csv("25_final_max10indv.csv", sep=";")
conStruct.coords <- data.matrix(sample.coords[2:3], rownames.force = NA)

# Making geographic distance matrix

conStruct.distances <- distm(conStruct.coords, fun = distGeo) # zeros on diagonal unlike in rdist.earth (due to rounding errors of the functions)

### CROSS-VALIDATION ###
# This takes a long time to run (more than 1 week without parallelisation), make sure you have the resources available (e.g. as a supercomputer batch job). 
# Options for parallelisation are available in the package.

my.xvals <- x.validation(train.prop = 0.9,
                         n.reps = 4,
                         K = 1:5,
                         freqs = conStruct.allele.freq,
                         data.partitions = NULL,
                         geoDist = conStruct.distances,
                         coords = conStruct.coords,
                         prefix = "crossvalidation",
                         n.iter = 5000,
                         make.figs = TRUE,
                         save.files = TRUE)

# PROCESSING THE RESULTS OF CONSTRUCT CROSS-VALIDATION

library(Rcpp)
library(conStruct)
library(geosphere)

# read in results from text files

sp.results <- as.matrix(
  read.table("crossvalidation_sp_xval_results.txt",
             header = TRUE,
             stringsAsFactors = FALSE)
)

nsp.results <- as.matrix(
  read.table("crossvalidation_nsp_xval_results.txt",
             header = TRUE,
             stringsAsFactors = FALSE)
)

# first, get the 95% confidence intervals for the spatial and nonspatial
#   models over values of K (mean +/- 1.96 the standard error)

sp.CIs <- apply(sp.results,1,function(x){mean(x) + c(-1.96,1.96) * sd(x)/length(x)})
nsp.CIs <- apply(nsp.results,1,function(x){mean(x) + c(-1.96,1.96) * sd(x)/length(x)})

# then, plot cross-validation results for K=1:3 with 4 replicates

par(mfrow=c(1,3))
plot(rowMeans(sp.results),
     pch=19,col="blue",
     ylab="predictive accuracy",xlab="values of K",
     ylim=range(sp.results,nsp.results),
     main="cross-validation results")
points(rowMeans(nsp.results),col="green",pch=19)

# finally, visualize results for the spatial model
#   separately with its confidence interval bars
#
# note that you could do the same with the spatial model, 
#   but the confidence intervals don't really show up 
#   because the differences between predictive accuracies
#   across values of K are so large.

plot(rowMeans(sp.results),
     pch=19,col="blue",
     ylab="predictive accuracy",xlab="values of K",
     ylim=range(sp.CIs),
     main="spatial cross-validation CIs")
segments(x0 = 1:nrow(sp.results),
         y0 = sp.CIs[1,],
         x1 = 1:nrow(sp.results),
         y1 = sp.CIs[2,],
         col = "blue",lwd=2)

plot(rowMeans(nsp.results),
     pch=19,col="green",
     ylab="predictive accuracy",xlab="values of K",
     ylim=range(nsp.CIs),
     main="nonspatial cross-validation CIs")
segments(x0 = 1:nrow(nsp.results),
         y0 = nsp.CIs[1,],
         x1 = 1:nrow(nsp.results),
         y1 = nsp.CIs[2,],
         col = "green",lwd=2)

# Loop through output files generated by conStruct 
#   runs with K=1 through 5 and calculate the 
#   layer contributions for each layer in each run  

layer.contributions <- matrix(NA,nrow=5,ncol=5)

# load the conStruct.results.Robj and data.block.Robj
#   files saved at the end of a conStruct run
load("crossvalidation_sp_rep1K1_conStruct.results.Robj")
load("crossvalidation_sp_rep1K1_data.block.Robj")

# calculate layer contributions for the spatial model
layer.contributions[,1] <- c(calculate.layer.contribution(conStruct.results[[1]],data.block),rep(0,4))
tmp <- conStruct.results[[1]]$MAP$admix.proportions

for(i in 2:5){
  # load the conStruct.results.Robj and data.block.Robj
  #   files saved at the end of a conStruct run
  load(sprintf("crossvalidation_sp_rep1K%s_conStruct.results.Robj",i))
  load(sprintf("crossvalidation_sp_rep1K%s_data.block.Robj",i))
  
  # match layers up across runs to keep plotting colors consistent
  #   for the same layers in different runs
  tmp.order <- match.layers.x.runs(tmp,conStruct.results[[1]]$MAP$admix.proportions)  
  
  # calculate layer contributions
  layer.contributions[,i] <- c(calculate.layer.contribution(conStruct.results=conStruct.results[[1]],
                                                            data.block=data.block,
                                                            layer.order=tmp.order),
                               rep(0,5-i))
  tmp <- conStruct.results[[1]]$MAP$admix.proportions[,tmp.order]
}

write.table(layer.contributions, "sp_layer_contributions.txt")

barplot(layer.contributions,
        col=c("blue", "red", "goldenrod1", "forestgreen", "darkorchid1"),
        xlab="",
        ylab="layer contributions",
        main="Spatial model",
        names.arg=paste0("K=",1:5))



# calculate layer contributions for the nonspatial model

load("crossvalidation_nsp_rep1K1_conStruct.results.Robj")
load("crossvalidation_nsp_rep1K1_data.block.Robj")

layer.contributions[,1] <- c(calculate.layer.contribution(conStruct.results[[1]],data.block),rep(0,4))
tmp <- conStruct.results[[1]]$MAP$admix.proportions

for(i in 2:5){
  # load the conStruct.results.Robj and data.block.Robj
  #   files saved at the end of a conStruct run
  load(sprintf("crossvalidation_nsp_rep1K%s_conStruct.results.Robj",i))
  load(sprintf("crossvalidation_nsp_rep1K%s_data.block.Robj",i))
  
  # match layers up across runs to keep plotting colors consistent
  #   for the same layers in different runs
  tmp.order <- match.layers.x.runs(tmp,conStruct.results[[1]]$MAP$admix.proportions)  
  
  # calculate layer contributions
  layer.contributions[,i] <- c(calculate.layer.contribution(conStruct.results=conStruct.results[[1]],
                                                            data.block=data.block,
                                                            layer.order=tmp.order),
                               rep(0,5-i))
  tmp <- conStruct.results[[1]]$MAP$admix.proportions[,tmp.order]
}

write.table(layer.contributions, "nsp_layer_contributions.txt")

barplot(layer.contributions,
        col=c("blue", "red", "goldenrod1", "forestgreen", "darkorchid1"),
        xlab="",
        ylab="layer contributions",
        main="Nonspatial model",
        names.arg=paste0("K=",1:5))


## RESULT PLOTS ##

# By default, conStruct's plotting code plots the ancestry proportions of each individual sample as a pie plot on the map.

# Since all samples from the same sampling site have the same coordinates, this leads to stacked pie plots where the visible "top" one is just the ancestry proportions of one sample per site. 

# For visualisation, we will calculate the average ancestry proportions per sampling site and plot those as a pie plot.

library(dplyr)

# Making a site vector, indicating which sampling site each row of data belongs to.
# The rows are in the same order as the samples in the input file.

site_vector <- c("CZLU","CZLU","CZLU","CZLU","CZLU","CZOS","CZOS","CZOS","CZOS","CZOS","DEDG","DEDG","DEDG","DEDG","DEDG","DEDG","DEDG","DEDG","DEDG","DEDG","DEMH","DEMH","DEMH","DEMH","DEMH","DEMH","DEMH","DEMH","DEMH","DK","DK","DK","DK","DK","DK","DK","DK","DK","DK","EE","EE","EE","EE","EE","EE","EE","EE","EE","EE","ES","ES","ES","ES","ES","ES","ES","ES","ES","ES","FIE","FIE","FIE","FIE","FIE","FIE","FIE","FIE","FIE","FIE","FIW","FIW","FIW","FIW","FIW","FR","FR","FR","FR","FR","FR","FR","FR","FR","HR","HR","HR","HR","HR","HR","HR","HR","HR","IT","IT","IT","IT","IT","IT","IT","IT","IT","IT","LT","LT","LT","LT","LT","LT","LT","LT","LT","LT","NO","NO","NO","NO","NO","NO","NO","NO","NO","NO","PL","PL","PL","PL","PL","PL","PL","PL","PL","PL","RODD","RODD","RODD","RODD","RODD","RODD","RODD","RODD","ROTR","ROTR","ROTR","ROTR","ROTR","ROTR","ROTR","ROTR","SE","SE","SE","SE","SE","SE","SK","SK","SK","SK","SK","SK","SK","SK","SK","TR","TR","TR","TR","TR","TR","TR","TR")

# Read in the results from the specific model & replicate (EDIT the input files for plotting different models/replicates)
load("crossvalidation_sp_rep1K2_conStruct.results.Robj")
load("crossvalidation_sp_rep1K2_data.block.Robj")

# Combine admixture, coords, and site info
admix <- as.data.frame(conStruct.results$chain_1$MAP$admix.proportions)
coords <- as.data.frame(data.block$coords)
admix$site <- site_vector
coords$site <- site_vector

# Get average admixture per site
admix.props.site <- admix %>%
  group_by(site) %>%
  summarise(across(starts_with("V"), mean)) %>%
  ungroup() %>%
  select(-site) %>%
  as.matrix()

# Get coordinates per site, takes just the first row as all samples from the same site have the same coordinates
coords.site <- coords %>%
  distinct(site, .keep_all = TRUE) %>%
  arrange(site) %>%
  select(-site) %>%
  as.matrix()

# Plot the averaged admixture pie plots on map
par(mfrow=c(1,1))

maps::map(xlim = range(coords.site[,1]) + c(-5,5),
          ylim = range(coords.site[,2]) + c(-2,2),
          col = "darkgray")

make.admix.pie.plot(admix.proportions = admix.props.site,
                    coords = coords.site,
                    radii = 3.5,
                    add = TRUE)


# Checking the mean phi ("shared drift") values for the two layers in the spK2 model

# From Gideon Bradburd: "That term governs the baseline amount of relatedness between any 
# two samples with 100% membership in the layer, regardless of the geographic distance between
# them. So, if that number is really small, it's more likely that the samples being placed
# in the same layer doesn't mean that much."

# Replicate 1/4
load("crossvalidation_sp_rep1K2_conStruct.results.Robj")
load("crossvalidation_sp_rep1K2_data.block.Robj")

conStruct.results[["chain_1"]][["MAP"]][["layer.params"]][["layer_1"]][["phi"]]
conStruct.results[["chain_1"]][["MAP"]][["layer.params"]][["layer_2"]][["phi"]]

# Replicate 2/4
load("crossvalidation_sp_rep2K2_conStruct.results.Robj")
load("crossvalidation_sp_rep2K2_data.block.Robj")

conStruct.results[["chain_1"]][["MAP"]][["layer.params"]][["layer_1"]][["phi"]]
conStruct.results[["chain_1"]][["MAP"]][["layer.params"]][["layer_2"]][["phi"]]

# Replicate 3/4
load("crossvalidation_sp_rep3K2_conStruct.results.Robj")
load("crossvalidation_sp_rep3K2_data.block.Robj")

conStruct.results[["chain_1"]][["MAP"]][["layer.params"]][["layer_1"]][["phi"]]
conStruct.results[["chain_1"]][["MAP"]][["layer.params"]][["layer_2"]][["phi"]]

# Replicate 4/4
load("crossvalidation_sp_rep4K2_conStruct.results.Robj")
load("crossvalidation_sp_rep4K2_data.block.Robj")

conStruct.results[["chain_1"]][["MAP"]][["layer.params"]][["layer_1"]][["phi"]]
conStruct.results[["chain_1"]][["MAP"]][["layer.params"]][["layer_2"]][["phi"]]
